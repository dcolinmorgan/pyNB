{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbe4e59",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.7' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Install package\n",
    "!uv pip install -e .\n",
    "\n",
    "# Import Required Libraries\n",
    "import sys, os, json, time, glob, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from analyze.Data import Data\n",
    "from datastruct.Network import Network\n",
    "from methods.lasso import Lasso\n",
    "from methods.lsco import LSCO\n",
    "from methods.tigress import TIGRESS\n",
    "from analyze.CompareModels import CompareModels\n",
    "from bootstrap.nb_fdr import NetworkBootstrap\n",
    "\n",
    "# Configuration\n",
    "OUTPUT_DIR = 'benchmark_results'\n",
    "DATASET_ROOT = os.path.expanduser('../GeneSPIDER2/data/gs-datasets/N50')\n",
    "NETWORK_ROOT = os.path.expanduser('../GeneSPIDER2/data/gs-networks')\n",
    "ZETAVEC = np.logspace(-6, 0, 30)\n",
    "N_INIT, N_BOOT, FDR = 10, 10, 5\n",
    "Path(OUTPUT_DIR).mkdir(exist_ok=True)\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a70681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def run_comparison_analysis(true_net, inferred_net):\n",
    "    comp = CompareModels(true_net, Network(inferred_net))\n",
    "    try:\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        tf = (true_net.A != 0).astype(float).flatten()\n",
    "        inf = np.abs(inferred_net.flatten())\n",
    "        auroc = roc_auc_score(tf, inf) if len(np.unique(tf)) > 1 and inf.sum() > 0 else 0.5\n",
    "    except: auroc = 0.5\n",
    "    return {'f1': comp.F1[0] if comp.F1 else 0, 'auroc': auroc, 'pre': comp.pre[0] if comp.pre else 0, 'sen': comp.sen[0] if comp.sen else 0, 'mcc': comp.MCC[0] if comp.MCC else 0}\n",
    "\n",
    "def find_network_file(network_dir, network_id):\n",
    "    for pattern in [f\"*{network_id}*.json\", f\"**/*ID{network_id}.json\", f\"**/*ID{network_id}*\", f\"**/*{network_id}.json\", f\"**/*{network_id}*\"]:\n",
    "        if (matches := list(Path(network_dir).rglob(pattern))): return matches[0]\n",
    "    return None\n",
    "\n",
    "def run_standard_method(method_name, data, zetavec):\n",
    "    start = time.time()\n",
    "    try:\n",
    "        if method_name == 'LASSO': A_3d, _ = Lasso(data, alpha_range=zetavec); A = A_3d[:, :, -1]\n",
    "        elif method_name == 'LSCO': A_3d, _ = LSCO(data, threshold_range=zetavec); A = A_3d[:, :, -1]\n",
    "        elif method_name == 'TIGRESS': A = TIGRESS(data)\n",
    "        else: raise ValueError(f\"Unknown method: {method_name}\")\n",
    "        return A, time.time() - start, None\n",
    "    except Exception as e: return None, time.time() - start, str(e)\n",
    "\n",
    "def run_nestboot_method(method_name, data, net, zetavec, n_init, n_boot, fdr, seed=42):\n",
    "    np.random.seed(seed); nb = NetworkBootstrap(); start = time.time()\n",
    "    def inf_meth(ds): \n",
    "        if method_name == 'LASSO': return Lasso(ds, alpha_range=zetavec)[0]\n",
    "        elif method_name == 'LSCO': return LSCO(ds, threshold_range=zetavec)[0]\n",
    "        else: raise ValueError(f\"Unknown method: {method_name}\")\n",
    "    results = nb.run_nestboot(data, inf_meth, n_init, n_boot, seed, {})\n",
    "    exec_time = time.time() - start\n",
    "    binary_net = np.zeros((data.data.N, data.data.N))\n",
    "    for idx, (gi, gj) in enumerate(zip(results.gene_i, results.gene_j)):\n",
    "        i, j = int(gi.split('_')[1]), int(gj.split('_')[1])\n",
    "        binary_net[i, j] = results.xnet[idx]\n",
    "    metrics = run_comparison_analysis(net, binary_net)\n",
    "    return {'method': f'{method_name}+NestBoot', 'n_init': n_init, 'n_boot': n_boot, 'fdr': fdr, 'time': exec_time, **metrics, 'density': (binary_net != 0).sum() / binary_net.size, 'support': results.support, 'fp_rate': results.fp_rate}\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1c0810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Benchmark\n",
    "dataset_files = sorted(glob.glob(os.path.join(DATASET_ROOT, \"*.json\")))\n",
    "print(f\"Found {len(dataset_files)} N50 datasets.\")\n",
    "all_results = []; results_file = Path(OUTPUT_DIR) / 'n50_benchmark_results.csv'\n",
    "\n",
    "processed_count = 0\n",
    "for dataset_path in dataset_files[:5]:  # Limit to first 5 for testing\n",
    "    dataset_filename = os.path.basename(dataset_path)\n",
    "    print(f\"\\nProcessing {dataset_filename}\")\n",
    "    try:\n",
    "        data = Data.from_json_file(dataset_path)\n",
    "        with open(dataset_path) as f: json_data = json.load(f)\n",
    "        network_id = json_data['obj_data']['network'].split('-ID')[-1]\n",
    "        network_path = find_network_file(NETWORK_ROOT, network_id)\n",
    "        if not network_path: print(f\"Network not found for {network_id}\"); continue\n",
    "        net = Network.from_json_file(str(network_path))\n",
    "        methods = ['TIGRESS', 'LASSO', 'LSCO']; nestboot_methods = ['LASSO', 'LSCO']\n",
    "        for method in methods:\n",
    "            print(f\"Running {method}...\")\n",
    "            inferred_net, exec_time, error = run_standard_method(method, data, ZETAVEC)\n",
    "            if inferred_net is not None:\n",
    "                metrics = run_comparison_analysis(net, inferred_net)\n",
    "                result = {'dataset': dataset_filename, 'network': os.path.basename(str(network_path)), 'method': method, 'execution_time': exec_time, 'f1_score': metrics['f1'], 'auroc': metrics['auroc'], 'precision': metrics['pre'], 'recall': metrics['sen'], 'mcc': metrics['mcc'], 'density': (inferred_net != 0).sum() / inferred_net.size}\n",
    "                all_results.append(result)\n",
    "            else: print(f\"{method} failed: {error}\")\n",
    "        for method in nestboot_methods:\n",
    "            print(f\"Running {method}+NestBoot...\")\n",
    "            result = run_nestboot_method(method, data, net, ZETAVEC, N_INIT, N_BOOT, FDR)\n",
    "            result.update({'dataset': dataset_filename, 'network': os.path.basename(str(network_path))})\n",
    "            all_results.append(result)\n",
    "        processed_count += 1\n",
    "        pd.DataFrame(all_results).to_csv(results_file, index=False)\n",
    "    except Exception as e: print(f\"Error processing {dataset_filename}: {e}\"); import traceback; traceback.print_exc()\n",
    "\n",
    "print(f\"Processed {processed_count} datasets. Results saved to {results_file}.\")\n",
    "\n",
    "# Summary\n",
    "if all_results:\n",
    "    df = pd.DataFrame(all_results)\n",
    "    print(\"\\nSummary by Method:\")\n",
    "    for method in df['method'].unique():\n",
    "        mr = df[df['method'] == method]\n",
    "        print(f\"{method}: F1 {mr['f1_score'].mean():.3f} ± {mr['f1_score'].std():.3f}, AUROC {mr['auroc'].mean():.3f} ± {mr['auroc'].std():.3f}, Time {mr['execution_time'].mean():.1f} ± {mr['execution_time'].std():.1f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
