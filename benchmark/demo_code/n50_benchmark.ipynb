{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbe4e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.insert(0, 'src')\n",
    "from analyze.Data import Data\n",
    "from datastruct.Network import Network\n",
    "from methods.lasso import Lasso\n",
    "from methods.lsco import LSCO\n",
    "from methods.tigress import TIGRESS\n",
    "from analyze.CompareModels import CompareModels\n",
    "from bootstrap.nb_fdr import NetworkBootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e081b840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "OUTPUT_DIR = 'benchmark_results'\n",
    "DATASET_ROOT = os.path.expanduser('../GeneSPIDER2/data/gs-datasets/N50')\n",
    "NETWORK_ROOT = os.path.expanduser('../GeneSPIDER2/data/gs-networks')\n",
    "ZETAVEC = np.logspace(-6, 0, 30)\n",
    "N_INIT = 10\n",
    "N_BOOT = 10\n",
    "FDR = 5\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(OUTPUT_DIR)\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Configuration set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a70681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def run_comparison_analysis(true_net, inferred_net):\n",
    "    comp = CompareModels(true_net, Network(inferred_net))\n",
    "    \n",
    "    try:\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        true_flat = (true_net.A != 0).astype(float).flatten()\n",
    "        inferred_flat = np.abs(inferred_net.flatten())\n",
    "        \n",
    "        if len(np.unique(true_flat)) > 1 and np.sum(inferred_flat) > 0:\n",
    "            auroc = roc_auc_score(true_flat, inferred_flat)\n",
    "        else:\n",
    "            auroc = 0.5\n",
    "    except:\n",
    "        auroc = 0.5\n",
    "    \n",
    "    return {\n",
    "        'f1_score': comp.F1[0] if len(comp.F1) > 0 else 0.0,\n",
    "        'auroc': auroc,\n",
    "        'precision': comp.pre[0] if len(comp.pre) > 0 else 0.0,\n",
    "        'recall': comp.sen[0] if len(comp.sen) > 0 else 0.0,\n",
    "        'mcc': comp.MCC[0] if len(comp.MCC) > 0 else 0.0\n",
    "    }\n",
    "\n",
    "def find_network_file(network_dir, network_id):\n",
    "    matches = list(Path(network_dir).rglob(f\"*{network_id}*.json\"))\n",
    "    if matches:\n",
    "        return matches[0]\n",
    "    \n",
    "    patterns = [\n",
    "        f\"**/*ID{network_id}.json\",\n",
    "        f\"**/*ID{network_id}*\",\n",
    "        f\"**/*{network_id}.json\",\n",
    "        f\"**/*{network_id}*\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = list(Path(network_dir).rglob(pattern))\n",
    "        if matches:\n",
    "            return matches[0]\n",
    "    \n",
    "    return None\n",
    "\n",
    "def run_standard_method(method_name, data, zetavec):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        if method_name == 'LASSO':\n",
    "            A_3d, _ = Lasso(data, alpha_range=zetavec)\n",
    "            A_final = A_3d[:, :, -1]\n",
    "        elif method_name == 'LSCO':\n",
    "            A_3d, _ = LSCO(data, threshold_range=zetavec)\n",
    "            A_final = A_3d[:, :, -1]\n",
    "        elif method_name == 'TIGRESS':\n",
    "            A_final = TIGRESS(data)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method_name}\")\n",
    "        \n",
    "        exec_time = time.time() - start_time\n",
    "        return A_final, exec_time, None\n",
    "    \n",
    "    except Exception as e:\n",
    "        exec_time = time.time() - start_time\n",
    "        return None, exec_time, str(e)\n",
    "\n",
    "def run_nestboot_method(method_name, data, net, zetavec, n_init, n_boot, fdr, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    nb = NetworkBootstrap()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    def inference_method(dataset, zetavec=None):\n",
    "        if method_name == 'LASSO':\n",
    "            A_3d, _ = Lasso(dataset, alpha_range=zetavec)\n",
    "            return A_3d\n",
    "        elif method_name == 'LSCO':\n",
    "            A_3d, _ = LSCO(dataset, threshold_range=zetavec)\n",
    "            return A_3d\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method_name}\")\n",
    "    \n",
    "    results = nb.run_nestboot(\n",
    "        dataset=data,\n",
    "        inference_method=lambda ds, **kwargs: inference_method(ds, zetavec),\n",
    "        nest_runs=n_init,\n",
    "        boot_runs=n_boot,\n",
    "        seed=seed,\n",
    "        method_kwargs={}\n",
    "    )\n",
    "    \n",
    "    exec_time = time.time() - start_time\n",
    "    \n",
    "    binary_network = np.zeros((data.data.N, data.data.N))\n",
    "    for idx, (gene_i, gene_j) in enumerate(zip(results.gene_i, results.gene_j)):\n",
    "        i = int(gene_i.split('_')[1])\n",
    "        j = int(gene_j.split('_')[1])\n",
    "        binary_network[i, j] = results.xnet[idx]\n",
    "    \n",
    "    metrics = run_comparison_analysis(net, binary_network)\n",
    "    \n",
    "    result = {\n",
    "        'method': f'{method_name}+NestBoot',\n",
    "        'n_init': n_init,\n",
    "        'n_boot': n_boot,\n",
    "        'fdr': fdr,\n",
    "        'time': exec_time,\n",
    "        'auroc': metrics['auroc'],\n",
    "        'f1': metrics['f1_score'],\n",
    "        'precision': metrics['precision'],\n",
    "        'recall': metrics['recall'],\n",
    "        'density': np.sum(binary_network != 0) / binary_network.size,\n",
    "        'support_threshold': results.support,\n",
    "        'fp_rate': results.fp_rate\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1c0810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Datasets\n",
    "dataset_files = sorted(glob.glob(os.path.join(DATASET_ROOT, \"*.json\")))\n",
    "print(f\"Found {len(dataset_files)} N50 datasets.\")\n",
    "\n",
    "# Initialize results\n",
    "all_results = []\n",
    "results_file = output_dir / 'n50_benchmark_results.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c697be68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Benchmark\n",
    "processed_count = 0\n",
    "\n",
    "for dataset_path in dataset_files[:5]:  # Limit to first 5 for testing\n",
    "    dataset_filename = os.path.basename(dataset_path)\n",
    "    print(f\"\\nProcessing {dataset_filename}\")\n",
    "    \n",
    "    try:\n",
    "        data = Data.from_json_file(dataset_path)\n",
    "        \n",
    "        # Extract network ID\n",
    "        with open(dataset_path, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "        network_id = json_data['obj_data']['network'].split('-ID')[-1]\n",
    "        \n",
    "        network_path = find_network_file(NETWORK_ROOT, network_id)\n",
    "        if not network_path:\n",
    "            print(f\"Network not found for {network_id}\")\n",
    "            continue\n",
    "        \n",
    "        net = Network.from_json_file(str(network_path))\n",
    "        \n",
    "        # Run methods\n",
    "        methods = ['TIGRESS', 'LASSO', 'LSCO']\n",
    "        nestboot_methods = ['LASSO', 'LSCO']\n",
    "        \n",
    "        for method in methods:\n",
    "            print(f\"Running {method}...\")\n",
    "            inferred_net, exec_time, error = run_standard_method(method, data, ZETAVEC)\n",
    "            if inferred_net is not None:\n",
    "                metrics = run_comparison_analysis(net, inferred_net)\n",
    "                result = {\n",
    "                    'dataset': dataset_filename,\n",
    "                    'network': os.path.basename(str(network_path)),\n",
    "                    'method': method,\n",
    "                    'execution_time': exec_time,\n",
    "                    'f1_score': metrics['f1_score'],\n",
    "                    'auroc': metrics['auroc'],\n",
    "                    'precision': metrics['precision'],\n",
    "                    'recall': metrics['recall'],\n",
    "                    'mcc': metrics['mcc'],\n",
    "                    'density': np.sum(inferred_net != 0) / inferred_net.size\n",
    "                }\n",
    "                all_results.append(result)\n",
    "            else:\n",
    "                print(f\"{method} failed: {error}\")\n",
    "        \n",
    "        for method in nestboot_methods:\n",
    "            print(f\"Running {method}+NestBoot...\")\n",
    "            result = run_nestboot_method(method, data, net, ZETAVEC, N_INIT, N_BOOT, FDR)\n",
    "            result['dataset'] = dataset_filename\n",
    "            result['network'] = os.path.basename(str(network_path))\n",
    "            all_results.append(result)\n",
    "        \n",
    "        processed_count += 1\n",
    "        \n",
    "        # Save intermediate results\n",
    "        df_results = pd.DataFrame(all_results)\n",
    "        df_results.to_csv(results_file, index=False)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {dataset_filename}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"Processed {processed_count} datasets.\")\n",
    "print(f\"Results saved to {results_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68894990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "if all_results:\n",
    "    df_results = pd.DataFrame(all_results)\n",
    "    print(\"\\nSummary by Method:\")\n",
    "    for method in df_results['method'].unique():\n",
    "        method_results = df_results[df_results['method'] == method]\n",
    "        print(f\"{method}:\")\n",
    "        print(f\"  F1: {method_results['f1_score'].mean():.3f} ± {method_results['f1_score'].std():.3f}\")\n",
    "        print(f\"  AUROC: {method_results['auroc'].mean():.3f} ± {method_results['auroc'].std():.3f}\")\n",
    "        print(f\"  Time: {method_results['execution_time'].mean():.1f} ± {method_results['execution_time'].std():.1f}s\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
